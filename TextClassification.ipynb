{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from pyvi.pyvi import ViTokenizer, ViPosTagger\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import os\n",
    "from sklearn.svm import LinearSVC\n",
    "from gensim import corpora, matutils\n",
    "from sklearn.metrics import classification_report\n",
    "import sys\n",
    "import codecs\n",
    "import json\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Downloads/Data/stopwords-nlp-vi.txt\n"
     ]
    }
   ],
   "source": [
    "DIR_PATH=\"/Users/mac/Downloads/Data\"\n",
    "DIR_PATH_TRAIN = os.path.join(DIR_PATH,'Train_Full')\n",
    "DIR_PATH_TEST = os.path.join(DIR_PATH,'Test_Full')\n",
    "DICTIONARY_PATH = os.path.join(DIR_PATH,'dictionary.txt')\n",
    "STOP_WORDS = os.path.join(DIR_PATH,'stopwords-nlp-vi.txt')\n",
    "SPECIAL_CHARACTER = '0123456789?…“”–%@$.,=+-!;/()*\"&^:#|\\n\\t\\''\n",
    "print(STOP_WORDS)\n",
    "label=['Van hoa','The gioi','Khoa hoc','Suc khoe','Chinh tri Xa hoi',\n",
    "       'Vi tinh','Kinh doanh','The thao','Phap luat','Doi song']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadData(object):\n",
    "    def __init__(self,dataPath,encoder=None):\n",
    "        self.dataPath = dataPath\n",
    "        self.encoder = encoder if encoder != None else 'utf-16le'\n",
    "        \n",
    "    def get_name_files(self):\n",
    "        class_file = os.listdir(self.dataPath)        \n",
    "        class_label=[]\n",
    "        folders=[]\n",
    "        for file in os.listdir(self.dataPath):\n",
    "            class_label.append(file)\n",
    "            folders.append(os.path.join(self.dataPath,file))\n",
    "        self.folders = folders\n",
    "        self.class_label=class_label\n",
    "        #print(self.class_label)\n",
    "        #print (self.folders)\n",
    "        \n",
    "    def get_data(self):\n",
    "        self.get_name_files()\n",
    "        data=[]\n",
    "        i=0\n",
    "        for label in self.class_label:\n",
    "            rand = random.randint(50,100)\n",
    "            print(label)\n",
    "            for file in os.listdir(self.folders[self.class_label.index(label)]):\n",
    "                #print(file)\n",
    "                i+=1\n",
    "                with open(os.path.join(self.folders[self.class_label.index(label)],file),'rb') as f:\n",
    "                    s = f.read()                  \n",
    "                data.append({\n",
    "                    \"label\":label,\n",
    "                    \"content\":s.decode(self.encoder) \n",
    "                })\n",
    "                if(i==rand): break\n",
    "            i=0\n",
    "            #if(i>100): break\n",
    "        self.data=data\n",
    "        print(len(data))\n",
    "        return data\n",
    "    \n",
    "    def read_stopwords(self):\n",
    "        with open(self.dataPath, 'r',encoding=\"utf-8\") as f:\n",
    "            stopwords = set([w.strip().replace(' ', '_') for w in f.readlines()])\n",
    "        return stopwords\n",
    "    \n",
    "    def load_dictionary(self):\n",
    "        return corpora.Dictionary.load_from_text(self.dataPath)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileStore(object):\n",
    "    def __init__(self, filePath, data = None):\n",
    "        self.filePath = filePath\n",
    "        self.data = data\n",
    "\n",
    "    def store_dictionary(self, dict_words):\n",
    "        dictionary = corpora.Dictionary(dict_words)\n",
    "        #dictionary.filter_extremes(no_below=1, no_above=0.1)\n",
    "        dictionary.filter_extremes(no_below=20, no_above=0.3)\n",
    "        dictionary.save_as_text(self.filePath)\n",
    "\n",
    "    def save_model(self,W,train_loss,train_acc,valid_loss,valid_acc,lr,reg):\n",
    "        temp = []\n",
    "        temp.append(lr)\n",
    "        temp.append(reg)\n",
    "        np.save(self.filePath+\"/W_SVM.npy\",W);\n",
    "        np.save(self.filePath+\"/train_loss.npy\",train_loss);\n",
    "        np.save(self.filePath+\"/train_acc.npy\",train_acc);\n",
    "        np.save(self.filePath+\"/valid_loss.npy\",valid_loss);\n",
    "        np.save(self.filePath+\"/valid_acc.npy\",valid_acc);\n",
    "        np.save(self.filePath+\"/lr_reg.npy\",temp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtraction(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __build_dictionary(self):\n",
    "        print ('Building dictionary')\n",
    "        dict_words = []\n",
    "        i = 0\n",
    "        for text in self.data:\n",
    "            i+=1\n",
    "            #print (\"Step {} / {}\".format(i, len(self.data)))\n",
    "            words = NLP(text=text['content']).get_words_feature()\n",
    "            dict_words.append(words)\n",
    "        FileStore(filePath=DICTIONARY_PATH).store_dictionary(dict_words)\n",
    "\n",
    "    def __load_dictionary(self):\n",
    "        if os.path.exists(DICTIONARY_PATH) == False:\n",
    "            self.__build_dictionary()\n",
    "        self.dictionary = ReadData(DICTIONARY_PATH).load_dictionary()\n",
    "\n",
    "    def build_dataset(self):\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "        i = 0\n",
    "        for d in self.data:\n",
    "            i += 1\n",
    "            #print (\"Step {} / {}\".format(i, len(self.data)))\n",
    "            self.features.append(self.get_dense(d['content']))\n",
    "            self.labels.append(label.index(d['label']))\n",
    "            #self.labels.append(d['label'])\n",
    "\n",
    "    def get_dense(self, text):\n",
    "        self.__load_dictionary()\n",
    "        words = NLP(text).get_words_feature()\n",
    "        i=0\n",
    "        #print(words)\n",
    "        # Bag of words\n",
    "        vec = self.dictionary.doc2bow(words)\n",
    "        dense = list(matutils.corpus2dense([vec], num_terms=len(self.dictionary)).T[0])\n",
    "        #return words,len(self.dictionary)\n",
    "        return dense\n",
    "\n",
    "    def get_data_and_label(self):\n",
    "        self.build_dataset()\n",
    "        return self.features, self.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP(object):\n",
    "    def __init__(self, text = None):\n",
    "        self.text = text\n",
    "        self.__set_stopwords()\n",
    "\n",
    "    def segmentation(self):\n",
    "        return ViTokenizer.tokenize(self.text)\n",
    "    \n",
    "    def display(self):\n",
    "        print(self.segmentation())\n",
    "        \n",
    "    def __set_stopwords(self):\n",
    "        self.stopwords = ReadData(STOP_WORDS).read_stopwords()\n",
    "        \n",
    "    def split_words(self):\n",
    "        text = self.segmentation()\n",
    "        try:\n",
    "            return [x.strip(SPECIAL_CHARACTER).lower() for x in text.split()]\n",
    "        except TypeError:\n",
    "            return []\n",
    "        \n",
    "    def get_words_feature(self):\n",
    "        split_words = self.split_words()\n",
    "        return [word for word in split_words if word.encode('utf-8') not in self.stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Van hoa\n",
      "The gioi\n",
      "Khoa hoc\n",
      "Suc khoe\n",
      "Chinh tri Xa hoi\n",
      "Vi tinh\n",
      "Kinh doanh\n",
      "The thao\n",
      "Phap luat\n",
      "Doi song\n",
      "710\n",
      "Van hoa\n",
      "The gioi\n",
      "Khoa hoc\n",
      "Suc khoe\n",
      "Chinh tri Xa hoi\n",
      "Vi tinh\n",
      "Kinh doanh\n",
      "The thao\n",
      "Phap luat\n",
      "Doi song\n",
      "775\n",
      "Building dictionary\n"
     ]
    }
   ],
   "source": [
    "read_train = ReadData(DIR_PATH_TRAIN)\n",
    "data_train = read_train.get_data()\n",
    "read_test = ReadData(DIR_PATH_TEST)\n",
    "data_test = read_test.get_data()\n",
    "x_train,y_train = FeatureExtraction(data=data_train).get_data_and_label()\n",
    "x_test,y_test = FeatureExtraction(data=data_test).get_data_and_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(710, 10)\n",
      "(775, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.asarray(y_train)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "y_train = onehot_encoder.fit_transform(y_train)\n",
    "print(y_train.shape)\n",
    "\n",
    "y_test = np.asarray(y_test)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "y_test = onehot_encoder.fit_transform(y_test)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(710, 1089)\n",
      "(775, 1089)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.asarray(x_train)\n",
    "x_train = np.hstack([x_train, np.ones((x_train.shape[0], 1))])\n",
    "x_test=np.asarray(x_test)\n",
    "x_test = np.hstack([x_test, np.ones((x_test.shape[0], 1))])\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568,)\n",
      "(142,)\n",
      "(568, 1089)\n",
      "(142, 1089)\n",
      "(568, 10)\n",
      "(142, 10)\n"
     ]
    }
   ],
   "source": [
    "train_numbers = np.random.choice(x_train.shape[0],round(x_train.shape[0]*0.8),replace=False)\n",
    "valid_numbers = np.array(list(set(range(x_train.shape[0])) - set(train_numbers)))\n",
    "print(train_numbers.shape)\n",
    "print(valid_numbers.shape)\n",
    "\n",
    "x_valid = x_train[valid_numbers]\n",
    "x_train= x_train[train_numbers]\n",
    "y_valid = y_train[valid_numbers]\n",
    "y_train = y_train[train_numbers]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-09dadf6ac750>:16: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "delta = 1.0\n",
    "regulation_rate = tf.placeholder(dtype=tf.float32,name=\"reg\")\n",
    "tf.summary.histogram(\"reg\", regulation_rate)\n",
    "learning_rate = tf.placeholder(dtype=tf.float32,name=\"learn\")\n",
    "tf.summary.histogram(\"learn\", learning_rate)\n",
    "\n",
    "x_data = tf.placeholder(shape=[None,x_train.shape[1]], dtype=tf.float32,name='x_data')\n",
    "#tf.summary.histogram(\"x_data\",x_data)\n",
    "y_data = tf.placeholder(shape=[None,y_train.shape[1]], dtype=tf.float32,name='y_data') \n",
    "#tf.summary.histogram(\"y_data\",y_data)\n",
    "W = tf.Variable(tf.random_normal(shape=[x_train.shape[1], y_train.shape[1]]),name='W_SVM')\n",
    "tf.summary.histogram(\"W_SVM\",W)\n",
    "    \n",
    "predict = tf.matmul(x_data,W)\n",
    "y = tf.reduce_sum(predict*y_data, 1, keep_dims=True)\n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.maximum(0.0, delta - y + predict),1))\n",
    "loss = tf.add(loss,regulation_rate * tf.nn.l2_loss(W))\n",
    "\n",
    "prediction = tf.argmax(predict,1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, tf.argmax(y_data,1)), tf.float32))\n",
    "tf.summary.scalar(\"loss\",loss)\n",
    "tf.summary.scalar(\"acc\",accuracy)\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = my_opt.minimize(loss)\n",
    "\n",
    "# Initialize variables\n",
    "# init = tf.global_variables_initializer()\n",
    "# sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "epoch: 0 - Loss_train: 109.98564 - Acc_train: 0.035714287 - Loss_valid: 73.06052 - Acc_valid: 0.15492958\n",
      "epoch: 10 - Loss_train: 42.947937 - Acc_train: 0.14285715 - Loss_valid: 45.128025 - Acc_valid: 0.23239437\n",
      "epoch: 20 - Loss_train: 25.968576 - Acc_train: 0.4642857 - Loss_valid: 34.908226 - Acc_valid: 0.33802816\n",
      "epoch: 30 - Loss_train: 8.46429 - Acc_train: 0.53571427 - Loss_valid: 29.797039 - Acc_valid: 0.38732395\n",
      "epoch: 40 - Loss_train: 6.3868194 - Acc_train: 0.5714286 - Loss_valid: 26.711388 - Acc_valid: 0.41549295\n",
      "epoch: 50 - Loss_train: 3.4228437 - Acc_train: 0.8214286 - Loss_valid: 24.946209 - Acc_valid: 0.44366196\n",
      "epoch: 60 - Loss_train: 2.7748508 - Acc_train: 0.78571427 - Loss_valid: 24.175535 - Acc_valid: 0.46478873\n",
      "epoch: 70 - Loss_train: 1.6261741 - Acc_train: 0.89285713 - Loss_valid: 23.803482 - Acc_valid: 0.47887325\n",
      "epoch: 80 - Loss_train: 1.0114987 - Acc_train: 1.0 - Loss_valid: 23.464106 - Acc_valid: 0.471831\n",
      "epoch: 90 - Loss_train: 1.0000056 - Acc_train: 1.0 - Loss_valid: 23.314234 - Acc_valid: 0.471831\n",
      "epoch: 100 - Loss_train: 1.0000056 - Acc_train: 1.0 - Loss_valid: 23.081432 - Acc_valid: 0.48591548\n",
      "epoch: 110 - Loss_train: 1.0000056 - Acc_train: 1.0 - Loss_valid: 23.000517 - Acc_valid: 0.48591548\n",
      "epoch: 120 - Loss_train: 1.0000056 - Acc_train: 1.0 - Loss_valid: 22.995014 - Acc_valid: 0.48591548\n",
      "early stopping...125\n",
      "Learning_rate: 0.001 - Regulariation: 1e-09 - Train_loss: 1.0000056- Valid_loss: 22.993492\n",
      "\n",
      "epoch: 0 - Loss_train: 74.21513 - Acc_train: 0.071428575 - Loss_valid: 101.36441 - Acc_valid: 0.09859155\n",
      "epoch: 10 - Loss_train: 39.094563 - Acc_train: 0.17857143 - Loss_valid: 58.282726 - Acc_valid: 0.20422535\n",
      "epoch: 20 - Loss_train: 11.79769 - Acc_train: 0.42857143 - Loss_valid: 42.04482 - Acc_valid: 0.26760563\n",
      "epoch: 30 - Loss_train: 13.293168 - Acc_train: 0.53571427 - Loss_valid: 34.768845 - Acc_valid: 0.33802816\n",
      "epoch: 40 - Loss_train: 9.474823 - Acc_train: 0.53571427 - Loss_valid: 31.165573 - Acc_valid: 0.37323943\n",
      "epoch: 50 - Loss_train: 3.3859956 - Acc_train: 0.8214286 - Loss_valid: 29.176157 - Acc_valid: 0.40140846\n",
      "epoch: 60 - Loss_train: 3.2166638 - Acc_train: 0.78571427 - Loss_valid: 28.208738 - Acc_valid: 0.43661973\n",
      "epoch: 70 - Loss_train: 1.7118114 - Acc_train: 0.9285714 - Loss_valid: 27.92841 - Acc_valid: 0.45070422\n",
      "epoch: 80 - Loss_train: 1.0270255 - Acc_train: 1.0 - Loss_valid: 27.670269 - Acc_valid: 0.45070422\n",
      "epoch: 90 - Loss_train: 1.1522747 - Acc_train: 0.96428573 - Loss_valid: 27.549227 - Acc_valid: 0.45070422\n",
      "epoch: 100 - Loss_train: 1.362021 - Acc_train: 0.96428573 - Loss_valid: 27.549286 - Acc_valid: 0.45774648\n",
      "epoch: 110 - Loss_train: 1.0794688 - Acc_train: 0.96428573 - Loss_valid: 27.473433 - Acc_valid: 0.45070422\n",
      "epoch: 120 - Loss_train: 1.0000542 - Acc_train: 1.0 - Loss_valid: 27.443027 - Acc_valid: 0.45070422\n",
      "epoch: 130 - Loss_train: 1.0000542 - Acc_train: 1.0 - Loss_valid: 27.416525 - Acc_valid: 0.45070422\n",
      "epoch: 140 - Loss_train: 1.0000542 - Acc_train: 1.0 - Loss_valid: 27.368168 - Acc_valid: 0.45070422\n",
      "early stopping...147\n",
      "Learning_rate: 0.001 - Regulariation: 1e-08 - Train_loss: 1.0000542- Valid_loss: 27.357592\n",
      "\n",
      "epoch: 0 - Loss_train: 57.158325 - Acc_train: 0.21428572 - Loss_valid: 76.86334 - Acc_valid: 0.09154929\n",
      "epoch: 10 - Loss_train: 21.438997 - Acc_train: 0.5 - Loss_valid: 39.550453 - Acc_valid: 0.2535211\n",
      "epoch: 20 - Loss_train: 14.696088 - Acc_train: 0.4642857 - Loss_valid: 29.194937 - Acc_valid: 0.36619717\n",
      "epoch: 30 - Loss_train: 5.6015806 - Acc_train: 0.6785714 - Loss_valid: 25.029823 - Acc_valid: 0.40140846\n",
      "epoch: 40 - Loss_train: 4.3887844 - Acc_train: 0.89285713 - Loss_valid: 23.214571 - Acc_valid: 0.40140846\n",
      "epoch: 50 - Loss_train: 3.948301 - Acc_train: 0.78571427 - Loss_valid: 22.59814 - Acc_valid: 0.4225352\n",
      "epoch: 60 - Loss_train: 1.5786217 - Acc_train: 0.9285714 - Loss_valid: 22.174349 - Acc_valid: 0.43661973\n",
      "epoch: 70 - Loss_train: 2.801864 - Acc_train: 0.9285714 - Loss_valid: 21.909855 - Acc_valid: 0.44366196\n",
      "epoch: 80 - Loss_train: 1.0055301 - Acc_train: 1.0 - Loss_valid: 21.743668 - Acc_valid: 0.44366196\n",
      "epoch: 90 - Loss_train: 1.2109747 - Acc_train: 0.96428573 - Loss_valid: 21.622763 - Acc_valid: 0.44366196\n",
      "epoch: 100 - Loss_train: 1.0000055 - Acc_train: 1.0 - Loss_valid: 21.506428 - Acc_valid: 0.45070422\n",
      "early stopping...109\n",
      "Learning_rate: 0.0015 - Regulariation: 1e-09 - Train_loss: 1.0000055- Valid_loss: 21.502777\n",
      "\n",
      "epoch: 0 - Loss_train: 52.152317 - Acc_train: 0.10714286 - Loss_valid: 114.30785 - Acc_valid: 0.07042254\n",
      "epoch: 10 - Loss_train: 23.794899 - Acc_train: 0.25 - Loss_valid: 52.754124 - Acc_valid: 0.2112676\n",
      "epoch: 20 - Loss_train: 11.713333 - Acc_train: 0.4642857 - Loss_valid: 38.30242 - Acc_valid: 0.2746479\n",
      "epoch: 30 - Loss_train: 6.435776 - Acc_train: 0.6785714 - Loss_valid: 32.917194 - Acc_valid: 0.31690142\n",
      "epoch: 40 - Loss_train: 1.8925277 - Acc_train: 0.89285713 - Loss_valid: 30.719057 - Acc_valid: 0.35915494\n",
      "epoch: 50 - Loss_train: 1.4759676 - Acc_train: 0.85714287 - Loss_valid: 29.530432 - Acc_valid: 0.37323943\n",
      "epoch: 60 - Loss_train: 1.1252966 - Acc_train: 0.96428573 - Loss_valid: 28.86092 - Acc_valid: 0.38732395\n",
      "epoch: 70 - Loss_train: 1.0000558 - Acc_train: 1.0 - Loss_valid: 28.614952 - Acc_valid: 0.38732395\n",
      "epoch: 80 - Loss_train: 1.0000558 - Acc_train: 1.0 - Loss_valid: 28.34111 - Acc_valid: 0.38732395\n",
      "epoch: 90 - Loss_train: 1.0000558 - Acc_train: 1.0 - Loss_valid: 28.157188 - Acc_valid: 0.3943662\n",
      "epoch: 100 - Loss_train: 1.0012414 - Acc_train: 1.0 - Loss_valid: 28.110067 - Acc_valid: 0.40140846\n",
      "early stopping...110\n",
      "Learning_rate: 0.0015 - Regulariation: 1e-08 - Train_loss: 1.0000558- Valid_loss: 28.036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "# learning_rates = [0.001,0.0015,0.002]\n",
    "# regularization_strengths = [1e-9,1e-8,1e-7]\n",
    "learning_rates = [0.001,0.0015]\n",
    "regularization_strengths = [1e-9,1e-8]\n",
    "loss_best=1000\n",
    "epoch=1000\n",
    "PATH='/Users/mac/Downloads/Data/logs/'\n",
    "patience_cnt = 0\n",
    "patience = 3\n",
    "min_delta = 0.001\n",
    "for lr in learning_rates:\n",
    "    for reg in regularization_strengths:     \n",
    "        sess.run(tf.initialize_all_variables()) \n",
    "        merged=tf.summary.merge_all()\n",
    "        history_valid_loss=[]\n",
    "#         train_loss = []\n",
    "#         valid_loss = []\n",
    "#         train_accuracy = []\n",
    "#         valid_accuracy = []\n",
    "        train_writer = tf.summary.FileWriter(PATH+'train'+'_'+str(lr)+'_'+str(reg),sess.graph)\n",
    "        valid_writer = tf.summary.FileWriter(PATH+'valid'+'_'+str(lr)+'_'+str(reg),sess.graph)\n",
    "        for it in range(epoch): \n",
    "            mix_ids = np.random.permutation(x_train.shape[0])\n",
    "            n_batches = int (np.ceil(x_train.shape[0]/float(batch_size)))\n",
    "            for ib in range(n_batches):\n",
    "                rand_index = mix_ids[batch_size*ib:min(batch_size*(ib+1),x_train.shape[0])]\n",
    "                rand_x = x_train[rand_index]\n",
    "                rand_y = y_train[rand_index,:]\n",
    "                \n",
    "                summary,batch_loss_train,acc_train,_ = sess.run([merged,loss,accuracy,train_step],\n",
    "                                                    feed_dict={x_data: rand_x, y_data: rand_y, learning_rate:lr, regulation_rate:reg})\n",
    "                train_writer.add_summary(summary,  it * n_batches + ib)\n",
    "        \n",
    "                summary,batch_loss_valid,acc_valid,= sess.run([merged,loss,accuracy],\n",
    "                                                  feed_dict={x_data: x_valid, y_data: y_valid, learning_rate: lr,regulation_rate: reg})\n",
    "                valid_writer.add_summary(summary,  it * n_batches + ib)\n",
    "    \n",
    "                sess.run(train_step, feed_dict={x_data: rand_x, y_data: rand_y,learning_rate:lr,regulation_rate:reg})\n",
    "\n",
    "#                 tloss = sess.run(loss, feed_dict={x_data: rand_x, y_data: rand_y,regulation_rate:reg})\n",
    "#                 train_loss.append(tloss)   \n",
    "    \n",
    "#                 train_acc_temp = sess.run(accuracy, feed_dict={\n",
    "#                     x_data: x_train,\n",
    "#                     y_data: y_train})\n",
    "#                 train_accuracy.append(train_acc_temp)   \n",
    "            \n",
    "#                 vloss = sess.run(loss, feed_dict={x_data: x_valid, y_data: y_valid,regulation_rate:reg})\n",
    "#                 valid_loss.append(vloss#)\n",
    "    \n",
    "#                 valid_acc_temp = sess.run(accuracy, feed_dict={\n",
    "#                     x_data: x_valid,\n",
    "#                     y_data: y_valid})\n",
    "#                 valid_accuracy.append(valid_acc_temp) \n",
    "#            if ((it) % 10 == 0):\n",
    "#                 print('epoch: '+str(it)+ ' - Loss_train: '+ str(tloss) +' - Acc_train: ' + str(train_acc_temp)+' - Loss_valid: '+str(vloss) + ' - Acc_valid: '+str(valid_acc_temp))\n",
    "#         print(\"Learning_rate: \"+str(lr) + \" - Regulariation: \"+ str(reg) + \" - Train_loss: \"+str(train_loss[-1]) +\"- Valid_loss: \"+ str(valid_loss[-1])+\"\\n\")\n",
    "            history_valid_loss.append(batch_loss_valid)\n",
    "            if (it > 0 and history_valid_loss[it-1] - history_valid_loss[it] > min_delta):\n",
    "                patience_cnt = 0\n",
    "            else:\n",
    "                patience_cnt = patience_cnt + 1\n",
    "            if (patience_cnt > patience):\n",
    "                print(\"early stopping...\"+str(it))\n",
    "                patience_cnt = 0\n",
    "                break\n",
    "            if ((it) % 10 == 0):\n",
    "                print('epoch: '+str(it)+ ' - Loss_train: '+ str(batch_loss_train) +' - Acc_train: ' \n",
    "                          + str(acc_train)+' - Loss_valid: '+str(batch_loss_valid) + ' - Acc_valid: '+str(acc_valid))\n",
    "        print(\"Learning_rate: \"+str(lr) + \" - Regulariation: \"+ str(reg) + \" - Train_loss: \"+str(batch_loss_train) +\" - Valid_loss: \"+ str(batch_loss_valid)+\"\\n\")\n",
    "        if (batch_loss_valid < loss_best):\n",
    "            loss_best = batch_loss_valid\n",
    "            W_best = sess.run(W)\n",
    "            lr_best = lr\n",
    "            reg_best = reg\n",
    "#             train_loss_best = train_loss\n",
    "#             valid_loss_best = valid_loss\n",
    "#             train_accuracy_best = train_accuracy\n",
    "#             valid_accuracy_best = valid_accuracy\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0015\n",
      "1e-09\n",
      "[[ 0.37619793 -0.94150376  0.05629951 ... -0.11769502  0.95302576\n",
      "  -0.24091767]\n",
      " [ 0.06807048 -0.3560431   0.20916696 ... -0.961757    0.32712042\n",
      "   1.4928484 ]\n",
      " [-0.11650599  0.44431412 -0.6500226  ...  0.3091188  -1.7611626\n",
      "  -1.9185907 ]\n",
      " ...\n",
      " [-0.48214805  0.8966015  -0.58215815 ... -0.24295774  0.11011754\n",
      "  -0.78113973]\n",
      " [-0.7783008  -0.8375448  -1.3911303  ... -0.25684366 -0.14985588\n",
      "  -0.33320722]\n",
      " [ 1.2287462  -0.1742637   1.2031218  ...  2.193503    0.13959922\n",
      "  -1.8080194 ]]\n",
      "50.45161290322581\n"
     ]
    }
   ],
   "source": [
    "print(lr_best)\n",
    "print(reg_best)\n",
    "print(W_best)\n",
    "def test(x_data,y_data,W):\n",
    "    y_pred = x_data.dot(W)\n",
    "    acc = np.mean(np.argmax(y_pred,1) == np.argmax(y_data,1))\n",
    "    return acc*100;\n",
    "accuracy=test(x_test,y_test,W_best)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_accuracy_best, 'k-', label='Training Accuracy')\n",
    "plt.plot(valid_accuracy_best, 'r--', label='Valid Accuracy')\n",
    "plt.title('Train And Valid Set Accuracies')\n",
    "plt.xlabel('Number Of iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss over time\n",
    "plt.plot(train_loss_best, 'k-',label='Training Loss')\n",
    "plt.plot(valid_loss_best, 'r--', label='Valid Loss')\n",
    "plt.title('Train And Valid Set Losses')\n",
    "plt.xlabel('Number Of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "print(\"learning_rate:\"+str(lr_best),\"regulation: \"+str(reg_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x_data,y_data,W):\n",
    "    y_pred = x_data.dot(W)\n",
    "    acc = np.mean(np.argmax(y_pred,1) == np.argmax(y_data,1))\n",
    "    return acc*100;\n",
    "accuracy=test(x_test,y_test,W_best)\n",
    "print(accuracy)\n",
    "\n",
    "save_model = FileStore(filePath=DIR_PATH).save_model(W_best,train_loss_best,\n",
    "                                                     train_accuracy_best,valid_loss_best,valid_accuracy_best,lr_best,reg_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1= np.load(DIR_PATH+\"/train_loss.npy\")\n",
    "temp2= np.load(DIR_PATH+\"/train_acc.npy\")\n",
    "temp3= np.load(DIR_PATH+\"/valid_loss.npy\")\n",
    "temp4= np.load(DIR_PATH+\"/valid_acc.npy\")\n",
    "temp5= np.load(DIR_PATH+\"/lr_reg.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = temp1.tolist()\n",
    "temp2 = temp2.tolist()\n",
    "temp3 = temp3.tolist()\n",
    "temp4 = temp4.tolist()\n",
    "temp5 = temp5.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(temp2, 'k-', label='Training Accuracy')\n",
    "plt.plot(temp4, 'r--', label='Valid Accuracy')\n",
    "plt.title('Train And Valid Set Accuracies')\n",
    "plt.xlabel('Number Of iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss over time\n",
    "plt.plot(temp1, 'k-',label='Training Loss')\n",
    "plt.plot(temp3, 'r--', label='Valid Loss')\n",
    "plt.title('Train And Valid Set Losses')\n",
    "plt.xlabel('Number Of Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "print(\"learning_rate:\"+str(temp5[0]),\"regulation: \"+str(temp5[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from gensim import corpora, matutils\n",
    "from pyvi.pyvi import ViTokenizer\n",
    "\n",
    "import tkinter as tk\n",
    "#from tkinter import ttk\n",
    "\n",
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "from tkinter.ttk import Frame, Button, Label, Style\n",
    "from tkinter import messagebox\n",
    "\n",
    "import re\n",
    "#import pickle as cPickle\n",
    "import numpy as np\n",
    "\n",
    "LARGE_FONT = (\"Verdana\", 12)\n",
    "\n",
    "class FileReader(object):\n",
    "\tdef __init__(self, filePath, encoder = None):\n",
    "\t\tself.filePath = filePath\n",
    "\t\tself.encoder = encoder if encoder != None else 'utf-16le'\n",
    "\n",
    "\tdef read(self):\n",
    "\t\ttry:\n",
    "\t\t\twith codecs.open(self.filePath, \"r\", \"utf-8\") as f:\n",
    "\t\t\t\ts = f.read()\n",
    "\t\texcept Exception:\n",
    "\t\t\twith codecs.open(self.filePath, \"r\", self.encoder) as f:\n",
    "\t\t\t\ts = f.read()\n",
    "\t\treturn s\n",
    "\tdef read_stopwords(self):\n",
    "\t\twith open(self.filePath, 'r', encoding=\"utf-8\") as f:\n",
    "\t\t\tstopwords = set([w.strip().replace(' ', '_') for w in f.readlines()])\n",
    "\t\treturn stopwords\n",
    "\n",
    "\tdef load_dictionary(self):\n",
    "\t\treturn corpora.Dictionary.load_from_text(self.filePath)\n",
    "\n",
    "class NLP(object):\n",
    "\tdef __init__(self, text = None):\n",
    "\t\tself.text = text\n",
    "\t\tself.__set_stopwords()\n",
    "\n",
    "\tdef __set_stopwords(self):\n",
    "\t\tself.stopwords = FileReader('stopwords-nlp-vi.txt').read_stopwords()\n",
    "\n",
    "\tdef segmentation(self):\n",
    "\t\treturn ViTokenizer.tokenize(self.text)\n",
    "\n",
    "\tdef split_words(self):\n",
    "\t\ttext = self.segmentation()\n",
    "\t\ttry:\n",
    "\t\t\treturn [x.strip('01234”56789%@$.,“?=+-!;/()–*’…\"&^:#|\\n\\t\\'').lower() for x in text.split()]\n",
    "\t\texcept TypeError:\n",
    "\t\t\treturn []\n",
    "\n",
    "\tdef get_words_feature(self):\n",
    "\t\tsplit_words = self.split_words()\n",
    "\t\treturn [word for word in split_words if word.encode('utf-8') not in self.stopwords]\n",
    "\n",
    "class FeatureExtraction(object):\n",
    "\tdef __load_dictionary(self):\n",
    "\t\tself.dictionary = FileReader('dictionary/dictionary.txt').load_dictionary()\n",
    "\n",
    "\tdef get_dense(self, text):\n",
    "\t\tself.__load_dictionary()\n",
    "\t\twords = NLP(text).get_words_feature()\n",
    "\t\tvec = self.dictionary.doc2bow(words)\n",
    "\t\tdense = list(matutils.corpus2dense([vec], num_terms=len(self.dictionary)).T[0])\n",
    "\t\treturn dense\n",
    "\n",
    "class Window(Frame):\n",
    "\tdef __init__(self, parent = None):\n",
    "\t\tFrame.__init__(self, parent)\n",
    "\t\tself.parent = parent\n",
    "\t\tself.path_file = None\n",
    "\t\tself.text_predict = None\n",
    "\t\tself.init_window()\n",
    "\t\t#with open('trained_model/my_dumped_classifier.pkl', 'rb') as fid:\n",
    "\t\t#\tmodel = cPickle.load(fid)\n",
    "\t\t#self.model = model\n",
    "\t\tself.W_best = np.load('weight/weight.npy')\n",
    "\t\tself.label = ['Văn hóa', 'Thế giới', 'Khoa học', 'Sức khỏe', 'Chính trị xã hội',\n",
    "\t\t'Vi tính', 'Kinh doanh', 'Thể thao', 'Pháp luật', 'Đời sống']\n",
    "\n",
    "\tdef init_window(self):\n",
    "\t\tself.parent.title(\"Classfication\")\n",
    "\n",
    "\t\tself.style = Style()\n",
    "\t\tself.style.theme_use(\"clam\")\n",
    "\n",
    "\t\tself.pack(fill = BOTH, expand = 1)\n",
    "\n",
    "\t\tquitButton = Button(self, text = 'Quit', command = self.close_window)\n",
    "\t\t#quitButton.config(font = BUTTON_FONT)\n",
    "\t\tquitButton.place(x = 0, y = 0)\n",
    "\n",
    "\t\tlabelBrowse = Label(self, text = 'Chọn file .txt dự đoán: ', font = LARGE_FONT)\n",
    "\t\tlabelBrowse.place(x = 50, y = 50)\n",
    "\n",
    "\t\tbrowseButton = Button(self, text = 'Browse file', command = self.browse_file)\n",
    "\t\tbrowseButton.place(x = 240, y = 45)\n",
    "\n",
    "\t\tself.text = Label(self, text = self.path_file, font = LARGE_FONT)\n",
    "\t\tself.text.place(x = 340, y = 50)\n",
    "\n",
    "\t\tpredictButton = Button(self, text = 'Predict', command = self.predict)\n",
    "\t\tpredictButton.place(x = 240, y = 80)\n",
    "\n",
    "\t\tself.textBox = Text(self, height = 24, width = 94, font = LARGE_FONT)\n",
    "\t\tscroll = Scrollbar(self)\n",
    "\t\t#scroll.pack(side = RIGHT, fill = Y)\n",
    "\t\t#self.textBox.pack(side = LEFT, fill = Y)\n",
    "\t\tscroll.config(command = self.textBox.yview)\n",
    "\t\t#self.textBox.config(yscrollcommand = scroll.set)\n",
    "\t\t#test = \"affffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff\"\n",
    "\t\t#self.textBox.insert(END, test)\n",
    "\t\tself.textBox.place(x = 80, y = 120)\n",
    "\t\tself.textBox.config(state = DISABLED)\n",
    "\n",
    "\t\tself.predictLabel = Label(self, text = self.text_predict, font = LARGE_FONT)\n",
    "\t\tself.predictLabel.place(x = 340, y = 87)\n",
    "\n",
    "\tdef close_window(self):\n",
    "\t\tclick = messagebox.askquestion(\"Close Window\", \"Are You Sure?\", icon = 'warning')\n",
    "\t\tif click == 'yes':\n",
    "\t\t\texit()\n",
    "\n",
    "\tdef browse_file(self):\n",
    "\t\tfilename = filedialog.askopenfilename()\n",
    "\t\tself.path_file = filename\n",
    "\t\tself.text.config(text = filename)\n",
    "\n",
    "\t\tisTxt = re.findall(r\"\\.\\w{1,}\", filename)\n",
    "\t\tif len(isTxt) == 0:\n",
    "\t\t\tmessagebox.showinfo(title = \"Error\", message = \"Lỗi định dạng file\")\n",
    "\t\t\tself.textBox.config(state = NORMAL)\n",
    "\t\t\tself.textBox.delete('1.0', END)\n",
    "\t\t\tself.textBox.config(state = DISABLED)\n",
    "\t\t\tself.content = \"\";\n",
    "\t\telse:\n",
    "\t\t\tif isTxt[0] == \".txt\":\n",
    "\t\t\t\ttxt = FileReader(filename).read()\n",
    "\t\t\t\tself.textBox.config(state = NORMAL)\n",
    "\t\t\t\tself.textBox.delete('1.0', END)\n",
    "\t\t\t\tself.textBox.insert(END, txt)\n",
    "\t\t\t\tself.textBox.config(state = DISABLED)\n",
    "\t\t\t\tself.content = txt;\n",
    "\t\t\t\t#messagebox.showinfo(title = \"Infomation\", message = txt)\n",
    "\t\t\telse:\n",
    "\t\t\t\tmessagebox.showinfo(title = \"Error\", message = \"Lỗi định dạng file\")\n",
    "\t\t\t\tself.textBox.config(state = NORMAL)\n",
    "\t\t\t\tself.textBox.delete('1.0', END)\n",
    "\t\t\t\tself.textBox.config(state = DISABLED)\n",
    "\t\t\t\tself.content = \"\";\n",
    "\t\t#messagebox.showinfo(title = 'Infomation', message = filename)\n",
    "\n",
    "\tdef predict(self):\n",
    "\t\tif self.content != \"\":\n",
    "\t\t\tself.dense = FeatureExtraction().get_dense(self.content)\n",
    "\t\t\t#vector = np.reshape(self.dense, (-1, 1)).T\n",
    "\t\t\t#label_predict = self.model.predict(vector)\n",
    "\t\t\t#self.predictLabel.config(text = label_predict[0])\n",
    "\t\t\tx_predict = np.asarray(self.dense)\n",
    "\t\t\tx_predict = np.hstack([x_predict, 1])\n",
    "\t\t\ty_predict = x_predict.dot(self.W_best)\n",
    "\t\t\tindex_predict = np.argmax(y_predict)\n",
    "\t\t\tself.predictLabel.config(text = self.label[index_predict])\n",
    "\t\telse:\n",
    "\t\t\tself.predictLabel.config(text = \"\")\n",
    "\t\t\tmessagebox.showinfo(title = \"Error\", message = \"Vui lòng chọn file\")\n",
    "\n",
    "def disable_event():\n",
    "\tpass\n",
    "\n",
    "root = Tk()\n",
    "root.geometry(\"1100x600+130+50\")\n",
    "root.protocol(\"WM_DELETE_WINDOW\", disable_event)\n",
    "root.resizable(0, 0)\n",
    "app = Window(root)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess.run(W)\n",
    "saver.save(sess,\"/Users/mac/Downloads/Data/svm_model\",global_step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()    \n",
    "#First let's load meta graph and restore weights\n",
    "saver = tf.train.import_meta_graph('/Users/mac/Downloads/Data/svm_model-100.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('/Users/mac/Downloads/Data/'))\n",
    "\n",
    "# Access saved Variables directly\n",
    "print(sess.run('reg'))\n",
    "print(sess.run('learn'))\n",
    "# This will print 2, which is the value of bias that we saved\n",
    "\n",
    "# Now, let's access and create placeholders variables and\n",
    "# create feed-dict to feed new data\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "x = graph.get_tensor_by_name(\"x_data\")\n",
    "y = graph.get_tensor_by_name(\"y_data\")\n",
    "feed_dict ={x:x_test,y:y_test}\n",
    "\n",
    "#Now, access the op that you want to run. \n",
    "op_to_restore = graph.get_tensor_by_name(\"op_to_restore:0\")\n",
    "\n",
    "print (sess.run(op_to_restore,feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(50,100)\n",
    "Y = np.random.rand(50,10)\n",
    "batchsize=6\n",
    "numiters = 10\n",
    "for it in range(numiters):\n",
    "    mixids = np.random.permutation(X.shape[0])\n",
    "    print(mixids)\n",
    "    nbatches = int (np.ceil(X.shape[0]/float(batchsize)))\n",
    "    print(nbatches)\n",
    "    for ib in range(nbatches):\n",
    "        ids = mixids[batchsize*ib:min(batchsize*(ib+1),X.shape[0])]\n",
    "        print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "#feature_size = x_train.shape[1]\n",
    "delta = 1.0\n",
    "regulation_rate = 8e-4\n",
    "x_data = tf.placeholder(shape=[None,x_train.shape[1]], dtype=tf.float32)\n",
    "y_data = tf.placeholder(shape=[None,y_train.shape[1]], dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_normal(shape=[x_train.shape[1], y_train.shape[1]]))\n",
    "\n",
    "def svm_loss(x_data,y_data,W,regulation_rate):\n",
    "    # graph = tf.Graph()\n",
    "    # with graph.as_default():\n",
    "    predict = tf.matmul(x_data,W)\n",
    "    y = tf.reduce_sum(predict*y_data, 1, keep_dims=True)\n",
    "    #loss = tf.reduce_mean(tf.reduce_sum(tf.maximum(0.0, delta - y + predict),1))-delta\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(tf.maximum(0.0, delta - y + predict),1))\n",
    "    loss = tf.add(loss,regulation_rate * tf.nn.l2_loss(W))\n",
    " \n",
    "    return loss\n",
    "\n",
    "def accuracy(x_data,y_data,W):\n",
    "    predict = tf.matmul(x_data,W)\n",
    "    prediction = tf.argmax(predict,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, tf.argmax(y_data,1)), tf.float32))\n",
    "    return accuracy*100;\n",
    "\n",
    "def svm_multi_class(x_data,y_data,Winit,reg,lr=8e-4,batch_size = 100, num_iters = 1500):\n",
    "    W = Winit\n",
    "    my_opt = tf.train.AdamOptimizer(lr)\n",
    "    train_step = my_opt.minimize(loss)\n",
    "    for i in range(num_iters):\n",
    "        rand_index = np.random.choice(x_data.shape[0], size=batch_size)\n",
    "        rand_x = x_data[rand_index]\n",
    "        rand_y = y_data[rand_index,:]\n",
    "        sess.run(train_step, feed_dict={x_data: rand_x, y_data: rand_y})\n",
    "\n",
    "        tloss = sess.run(svm_loss, feed_dict={x_data: rand_x, y_data: rand_y,regulation_rate=lr})\n",
    "        train_loss.append(tloss)\n",
    "\n",
    "        Weight = sess.run(W)\n",
    "        vloss = sess.run(loss, feed_dict={x_data: x_valid, y_data: y_valid,W:Weight})\n",
    "        valid_loss.append(vloss)\n",
    "    \n",
    "        train_acc_temp = sess.run(accuracy, feed_dict={\n",
    "            x_data: x_train,\n",
    "            y_data: y_train})\n",
    "        train_accuracy.append(train_acc_temp)\n",
    "\n",
    "        valid_acc_temp = sess.run(accuracy, feed_dict={\n",
    "            x_data: x_valid,\n",
    "            y_data: y_valid})\n",
    "        valid_accuracy.append(valid_acc_temp)\n",
    "    return W,train_loss,train_accuracy,valid_loss,valid_accuracy    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
